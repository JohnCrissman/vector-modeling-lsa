{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CN2zxwTFKYhs",
        "HdE5x8S1Y9ix",
        "xfUgpHiiZPmM",
        "YIJj59aPSZnH"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPstuu9oc1FyBMqvZC0B7Zy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianelamin/vector-modeling-lsa/blob/master/similarity_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HekxFN2SQfG",
        "colab_type": "text"
      },
      "source": [
        "#Mounting my drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB_AokJfViuN",
        "colab_type": "text"
      },
      "source": [
        "To mount my google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-4lFa_LklUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz39CmEShOKe",
        "colab_type": "text"
      },
      "source": [
        "change directory to custom python module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9PDT9fta1RY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/drive/'My Drive'/NEIU-CS/'cs490 Masters Project'/notebook/nlu_helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Dv16qAhYxF",
        "colab_type": "text"
      },
      "source": [
        "#####check for working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty4Y35S2aPma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls /content/drive/'My Drive'/NEIU-CS/'cs490 Masters Project'/notebook/nlu_helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmhA4hG0a-ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovQ-37q0lc7N",
        "colab_type": "text"
      },
      "source": [
        "#Comparing two texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBZQvIvlw8V",
        "colab_type": "text"
      },
      "source": [
        "## Importing modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpdSDfV5Yvty",
        "colab_type": "text"
      },
      "source": [
        "#### Importing python modules\n",
        "These modules help us deal with the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOtyRwY8W9H6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "960b2c77-f34f-4745-e4bb-811ab1d62cee"
      },
      "source": [
        "from traceback import print_exc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from sklearn.feature_extraction import stop_words"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEXImrJY2BT",
        "colab_type": "text"
      },
      "source": [
        "####Importing custome modules\n",
        "* *utility* has functions.\n",
        "* *read_model* has the classes needed to load the model, process the text and compare two sentences using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukX6T4xMY8xF",
        "colab_type": "code",
        "outputId": "aa3e23de-5e99-4e75-df87-0ac1cb0dd368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PROJECT_PATH = os.path.dirname(os.path.abspath('.'))\n",
        "# print(sys.path)\n",
        "if PROJECT_PATH not in sys.path:\n",
        "    sys.path.append(PROJECT_PATH)\n",
        "\n",
        "from nlu_helper.read_model import MatrixModel, SentenceProcessor\n",
        "from nlu_helper.utility import load_json2py, create_a_logger, resources_folder, log_and_print, file2pickle\n",
        "from nlu_helper.utility import create_results_directory_if_needed\n",
        "\n",
        "print('Load modules from: ', PROJECT_PATH)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load modules from:  /content/drive/My Drive/NEIU-CS/cs490 Masters Project/notebook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FYRXemlus8M",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Using a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzd9tl8093IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check which models have already been uploaded into the google colab\n",
        "ls resources/all_senate_speeches/ | grep .json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8odFZM9SfZv",
        "colab_type": "text"
      },
      "source": [
        "A model is needed in order to compare two texts.  Down below there is a list of available models. These name of each models describes how it was generated.\n",
        "\n",
        "* `lsa` or `docbyterm`: implementing LSA or not\n",
        "* if `lsa` applied, `dim` should specify the number of dimensions\n",
        "* `count`, `tfidf` or `zeroone` defines the type of scoring when vectorizing the corpus. *count* refers to the frequency of the word in the document; *zeroone* to the existence or not of the word in the document; *tfidf* to the term frequecy inverse document frequency (importance of the word in the document).\n",
        "* `stop` if stopwords were removed from the corpus prior to the model generation\n",
        "* `RxC` *R* number of rows (words) in the model, *C* number of columns.\n",
        "\n",
        "From the recostructed matrix taking only the fisrt *C* columns.\n",
        "\n",
        "**!! Important !!**\n",
        "\n",
        "There is an issue with the memory.  This demo only works (in this notebook) for models that use LSA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZM0c0rhS3MZ",
        "colab_type": "text"
      },
      "source": [
        "### Selecting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skMA9whY799Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "cellView": "both",
        "outputId": "2e5dfcd2-7027-4dfb-d8f4-03bc21d32b83"
      },
      "source": [
        "model_filename = \"all_senate_speeches-min_df_2-lsa-dim100-count-stop_69414x1000\" #@param [\"all_senate_speeches-min_df_2-lsa-dim100-count_69724x1000\", \"all_senate_speeches-min_df_2-lsa-dim100-count-stop_69414x1000\", \"all_senate_speeches-min_df_2-lsa-dim100-tfidf_69724x1000\", \"all_senate_speeches-min_df_2-lsa-dim100-tfidf-stop_69414x1000\", \"all_senate_speeches-min_df_2-lsa-dim100-zeroone_69724x1000\", \"all_senate_speeches-min_df_2-lsa-dim100-zeroone-stop_69414x1000\", \"all_senate_speeches-min_df_2-lsa-dim300-count-stop_69414x1000\", \"all_senate_speeches-min_df_2-lsa-dim300-tfidf-stop_69414x1000\", \"all_senate_speeches-min_df_2-lsa-dim300-zeroone_69724x1000\"]\n",
        "\n",
        "model_filepath = 'all_senate_speeches/'+ model_filename\n",
        "print('file: ', model_filepath)\n",
        "\n",
        "\n",
        "try:\n",
        "  #create the matrix model, this reads and holds the data\n",
        "  matrix_model = MatrixModel(model_filepath, logger=None)\n",
        "\n",
        "  #create the processor object, this takes the model.\n",
        "  sp = SentenceProcessor(matrix_model)\n",
        "\n",
        "  print('\\t- MatrixModel loaded!')\n",
        "  print('\\t- SentenceProcessor ready!')\n",
        "except Exception:\n",
        "  print('There was an error with the model.  Either not found or you are attepting to use a non-lsa')\n",
        "  print_exc()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file:  all_senate_speeches/all_senate_speeches-min_df_2-lsa-dim100-count-stop_69414x1000\n",
            "\t- MatrixModel loaded!\n",
            "\t- SentenceProcessor ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTwrtxKhKP9l",
        "colab_type": "text"
      },
      "source": [
        "### 1. Using the model to compare\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBqo0lMdV8sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp.args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7E4TYj5Eo6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1= 'May the forth be with you'\n",
        "sentence2= 'cancer begings in the cells'\n",
        "\n",
        "s1_tokenized = sp.tokenize_sentence(sentence1)\n",
        "print(s1_tokenized)\n",
        "sentence1_vector = sp.get_sentence_vector(s1_tokenized)\n",
        "# print(sentence1_vector)\n",
        "\n",
        "s2_tokenized = sp.tokenize_sentence(sentence2)\n",
        "print(s2_tokenized)\n",
        "sentence2_vector = sp.get_sentence_vector(s2_tokenized)\n",
        "# print(sentence2_vector)\n",
        "\n",
        "sp.get_sentence_match(sentence1, sentence2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APrbmqsPVeRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check it the words are in English_stop_words set\n",
        "# print(stop_words.ENGLISH_STOP_WORDS)\n",
        "def which_were_removed(sentence):\n",
        "  print('\\nsentence: ', sentence)\n",
        "  for word in sentence.split():\n",
        "    print('\\t[x]' if word in stop_words.ENGLISH_STOP_WORDS else '\\t[ ]', word)\n",
        "\n",
        "which_were_removed(sentence1)\n",
        "which_were_removed(sentence2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2zxwTFKYhs",
        "colab_type": "text"
      },
      "source": [
        "### 2. Automatic multiple comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdE5x8S1Y9ix",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the Model Applier class.\n",
        "This is the class that will apply the model selected and provide us with the result. We do not need a log file, because everything will be printed on the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uanSoaM5ZOiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelApplier:\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        # self.logger = create_a_logger('read_model.log')\n",
        "        # self.logger.info('*************************************** STARTING TO APPLY MODEL')\n",
        "        # self.logger.info('model file used: ' + filename)\n",
        "        # self.logger.debug('creating the object of the MatrixModel')\n",
        "        self.logger = None\n",
        "\n",
        "        self.matrix_model = MatrixModel(filename, logger=self.logger)\n",
        "\n",
        "        # self.logger.debug('finish creating the MatrixModel object')\n",
        "        # print(matrix_model.args)\n",
        "        self.sp = SentenceProcessor(self.matrix_model, logger=self.logger)\n",
        "        # self.model_filename = filename\n",
        "\n",
        "    def paragraph_vs_sentences_similarities(self, real_answer: str, given_answers: list) -> list:\n",
        "        \"\"\"\n",
        "        :param real_answer: is a string containing the systems sentence/paragraph that we need to compare against\n",
        "        :param given_answers: is a list of the possible answers that could have been given by the human user\n",
        "        returns a list containing all cosine similarity values that were computed from comparing the answer to each\n",
        "         possible answer on the question dictionary\n",
        "        \"\"\"\n",
        "        r_tok = self.sp.tokenize_sentence(real_answer)\n",
        "        real_sentence_vector = self.sp.get_sentence_vector(r_tok)\n",
        "        array_sim = list()\n",
        "\n",
        "        for given_answer in given_answers:\n",
        "            # print(given_answer)\n",
        "            g_tok = self.sp.tokenize_sentence(given_answer['text'])\n",
        "            sim = self.sp.cos_sim(real_sentence_vector,\n",
        "                                  self.sp.get_sentence_vector(g_tok))\n",
        "            array_sim.append(sim)\n",
        "            # print('result: ', sim)\n",
        "\n",
        "        return array_sim\n",
        "\n",
        "    def sort_similarity(self, cosine_similarity_results, question: dict) -> dict:\n",
        "        # Add two more columns with 'id', 'text' tags\n",
        "        ids = [q['id'] for q in question['possible_answers']]\n",
        "        texts = [q['text'] for q in question['possible_answers']]\n",
        "        pdf = pd.DataFrame({'id': ids, 'similarity': np.asarray(cosine_similarity_results), 'text': texts},\n",
        "                           index=ids)\n",
        "\n",
        "        sorted_pdf = pdf.sort_values(by='similarity', ascending=False)\n",
        "        self.generate_log_similarities_report(question, sorted_pdf)\n",
        "\n",
        "        final_order = [i + 1 for i in range(len(sorted_pdf.values))]\n",
        "        sorted_pdf.insert(1, 'rank', final_order)\n",
        "\n",
        "        item = {\n",
        "            \"item_id\": question['lp_id'],\n",
        "            \"item_text\": question['answer'],\n",
        "            \"result_as_pdf\": json.loads(sorted_pdf.to_json()),\n",
        "            \"result_only_id\": sorted_pdf.index.tolist()\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def generate_log_similarities_report(self, question, pdf):\n",
        "        m = '\\n--------------------------------------- \\nRESULTS \\nCompare: ' + str(question['answer']) \\\n",
        "            + '\\nTo: \\n' + str(pdf) \\\n",
        "            + '\\n---------------------------------------'\n",
        "        log_and_print(m, self.logger)\n",
        "\n",
        "    def save_model_results(self, output_model_results, items, path_testfile):\n",
        "        with open(output_model_results + '.json', 'w') as fp:\n",
        "            json.dump({\n",
        "                \"testfile\": path_testfile,\n",
        "                \"model\": {\n",
        "                    \"filename\": self.matrix_model.filename,\n",
        "                    \"args\": self.matrix_model.args\n",
        "                },\n",
        "                \"items\": items\n",
        "            },\n",
        "                fp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfUgpHiiZPmM",
        "colab_type": "text"
      },
      "source": [
        "#### Defining multiple sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF2nvfWDs4CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questionaire = [\n",
        "                {\n",
        "                  \"lp_id\": \"LP2\",\n",
        "                  \"answer\": \"cancer begins in cells\",\n",
        "                  \"possible_answers\": [\n",
        "                      {\n",
        "                        \"id\": \"SP1\", \"text\": \"May the forth be with you.\"\n",
        "                      }\n",
        "                    ]\n",
        "                }\n",
        "              ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJj59aPSZnH",
        "colab_type": "text"
      },
      "source": [
        "#### Getting automatic ordered results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hPO7MoSScpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "546fa3d6-9cf0-4373-f5ab-fd68a833bfbb"
      },
      "source": [
        "print('Model to use: ', model_filepath)\n",
        "\n",
        "ma = ModelApplier(model_filepath)\n",
        "\n",
        "items = list()\n",
        "for question in questionaire:\n",
        "    possible_answers = question['possible_answers']\n",
        "    answer = question['answer']\n",
        "\n",
        "    if len(possible_answers) != 0:\n",
        "        cos_sim_results = ma.paragraph_vs_sentences_similarities(real_answer=answer,\n",
        "                                                                  given_answers=possible_answers)\n",
        "        print(cos_sim_results)\n",
        "        item = ma.sort_similarity(cos_sim_results, question)\n",
        "        items.append(item)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model to use:  all_senate_speeches/all_senate_speeches-min_df_2-lsa-dim100-tfidf-stop_69414x1000\n",
            "[0.014349999837577343]\n",
            "\n",
            "--------------------------------------- \n",
            "RESULTS \n",
            "Compare: cancer begins in cells\n",
            "To: \n",
            "      id  similarity                        text\n",
            "SP1  SP1     0.01435  May the forth be with you.\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNl-5p_tcA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2883b37c-fa27-47f7-ca5b-6017fc4e858f"
      },
      "source": [
        "print(items)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'item_id': 'LP2', 'item_text': 'members of america joined the russian Party.', 'result_as_pdf': {'id': {'SP1': 'SP1'}, 'rank': {'SP1': 1}, 'similarity': {'SP1': 0.8380200267}, 'text': {'SP1': 'In America, you can always find a party.'}}, 'result_only_id': ['SP1']}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}